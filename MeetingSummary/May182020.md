## Day
  - May, 18, 2020
## Timing
- https://www.worldtimebuddy.com/?qm=1&lid=30,1850147,6,14&h=30&date=2020-5-18&sln=20.5-21.5

## Discussed
- We have discussed on 4th chapter - Linear Regression and Gradient Descent variants.(till Page 127 of the book)
- We have seen ways of solving Linear Regression - Normal equation
- Affine Orthogonal Normalization
- Undetermined/ Over Determined - equations solving
- Cosine Similarity vs Cosine Distance
- Why Cos Theta in Cosine Similarity
- Normal Eqution is **not rewriting** of the equation **y(hat) = theta . x** (Will add latex markdown)
- Why dont we use Normal equation way of solving in day to day solutioning
- Computation Complexities behind Normal Equation
- Inverse / Pseudoinverse
- Gausian Elimination, Eigen Values, Eigen Vector
- Practical problems are in Non-Convex way
- Gradient Descent - its variants
- The Jacobian matrix and backpropogation- Partial Derivatives
- Effect of learning rate in Gradient Descent
- Learning rate's smoothness in Stochastic Gradient Descent, Mini-Batch GD and Batch GD
- Learning rate and Converging to minimum - saddle points or global minimum

## Activities:
- Experiment the curve for Mean Absolute Error, Mean Squared Error
- Explore convex and non convex functions

## To be explored - research focus
- Guaranteedness of the Converged Minimum is a Global Minimum
- How can we conclude that the model is converged at some point
